{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wo34QvYB5Jw"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets torchaudio librosa jiwer accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import torchaudio\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer\n",
        "\n",
        "# Load Whisper model and processor\n",
        "model_name = \"openai/whisper-small\"\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load custom dataset (modify path accordingly)\n",
        "dataset = load_dataset(\"path_to_medical_audio_dataset\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    batch[\"input_features\"] = input_features\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(preprocess_data, remove_columns=[\"audio\", \"transcription\"])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./whisper-medical\",\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"./whisper-medical\")\n",
        "processor.save_pretrained(\"./whisper-medical\")"
      ],
      "metadata": {
        "id": "dYkKPpZaEPqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate"
      ],
      "metadata": {
        "id": "MCuYoSh6ER8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# Load pre-trained GPT model (can use Llama-2, GPT-J, or GPT-3.5-like models)\n",
        "model_name = \"EleutherAI/gpt-j-6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the medical dataset (Modify path)\n",
        "dataset = load_dataset(\"path_to_medical_text_dataset\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"transcription\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt-medical\",\n",
        "    per_device_train_batch_size=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"./gpt-medical\")\n",
        "tokenizer.save_pretrained(\"./gpt-medical\")"
      ],
      "metadata": {
        "id": "K5yjbR9QEXyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}